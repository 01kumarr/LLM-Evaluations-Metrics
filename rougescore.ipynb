{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d746088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from typing import Union, List\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbafa8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROUGEScore:\n",
    "    \"\"\"\n",
    "    ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric implementation.\n",
    "    Focuses on recall - how much of reference content is captured in generated text.\n",
    "    \"\"\"\n",
    "    def _get_ngrams(self, tokens:List[str], n:int):\n",
    "        if n > len(tokens):\n",
    "            return Counter() \n",
    "        \n",
    "        ngrams = []\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            ngrams.append(tuple(tokens[i:i+n])) \n",
    "        return Counter(ngrams) \n",
    "    \n",
    "    def _lcs_length(self, x:List[str], y:List[str]):\n",
    "        \"\"\"Calculate length of longest common string\"\"\"\n",
    "        m,n = len(x), len(y) \n",
    "        dp = [[0]*(n+1) for _ in range(m+1)] \n",
    "\n",
    "        for i in range(1, m+1):\n",
    "            for j in range(1, n+1):\n",
    "                if x[i-1] == y[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1]+1 \n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1]) \n",
    "\n",
    "        return dp[m][n]\n",
    "    \n",
    "    def rouge_n(self, candidate:str, reference:str, n:int=1):\n",
    "        \"\"\"candidate: generated text,\n",
    "           references: reference text\n",
    "           n = n grams(default = 1)\n",
    "\n",
    "           Returns:\n",
    "                Dictionary with precision, recall and F1 scores\n",
    "        \"\"\"\n",
    "        candidate_tokens = candidate.lower().split() \n",
    "        reference_tokens = reference.lower().split()\n",
    "\n",
    "        candidate_ngrams = self._get_ngrams(candidate_tokens, n)\n",
    "        reference_ngrams = self._get_ngrams(reference_tokens, n) \n",
    "\n",
    "        if not reference_ngrams:\n",
    "            return {'precision':0.0, 'recall':0.0, 'f1': 0.0}\n",
    "\n",
    "        ## calculate the overlap\n",
    "        overlap = 0 \n",
    "        for ngram in candidate_ngrams:\n",
    "            overlap+= min(candidate_ngrams[ngram], reference_ngrams[ngram])\n",
    "\n",
    "        ## calculate - precision, recall, and f1 score\n",
    "        precision = overlap / sum(candidate_ngrams.values()) if candidate_ngrams else 0.0 # true positive out of predicted positive\n",
    "        recall = overlap / sum(reference_ngrams.values()) if reference_ngrams else 0.0  # true positive out of actual positive\n",
    "        f1 = (precision*recall) / (precision+recall) if (precision+recall) >0 else 0.0\n",
    "\n",
    "\n",
    "        return {'precision': precision, 'recall':recall, 'f1':f1} \n",
    "    \n",
    "    def rouge_l(self, candidate:str, reference:str):\n",
    "        \"\"\" Calculate rouge l using longest common string\"\"\" \n",
    "        candidate_tokens = candidate.lower().split() \n",
    "        reference_tokens = reference.lower().split() \n",
    "\n",
    "        if not reference_tokens:\n",
    "            return {'precision':0.0, 'recall':0.0, 'f1':0.0} \n",
    "        \n",
    "        lcs_length = self._lcs_length(candidate_tokens, reference_tokens)\n",
    "        precision = lcs_length / len(candidate_tokens) if candidate_tokens else 0.0 \n",
    "        recall = lcs_length / len(reference_tokens) if reference_tokens else 0.0 \n",
    "        f1 = (precision*recall) / (recall+precision) if (precision+recall) > 0 else 0.0 \n",
    "\n",
    "        return {'precision': precision, 'recall':recall, 'f1':f1}\n",
    "    \n",
    "    def calculate_all(self,candidate:str, reference:str):\n",
    "        return{\n",
    "            'rouge-1' : self.rouge_n(candidate, reference, 1),\n",
    "            'rouge-2' : self.rouge_n(candidate, reference, 2),\n",
    "            'rouge-l' : self.rouge_l(candidate, reference) \n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b21a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'precision': 0.7777777777777778, 'recall': 0.7, 'f1': 0.36842105263157887}, 'rouge-2': {'precision': 0.75, 'recall': 0.6666666666666666, 'f1': 0.35294117647058826}, 'rouge-l': {'precision': 0.7777777777777778, 'recall': 0.7, 'f1': 0.36842105263157887}}\n"
     ]
    }
   ],
   "source": [
    "## example test\n",
    "candidate = \"The cat sat on the mat and looked around\"\n",
    "reference_rouge = \"The cat sat on the mat and was very comfortable\" \n",
    "\n",
    "rouge_scorer = ROUGEScore()\n",
    "rouge_results = rouge_scorer.calculate_all(candidate, reference_rouge) \n",
    "print(rouge_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837387ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
