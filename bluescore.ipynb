{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a946a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## designing blue score for model response output with ground truth references\n",
    "import math \n",
    "from collections import Counter \n",
    "from typing import List \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEUScore:\n",
    "    \"\"\"\n",
    "    BLEU (Bilingual Evaluation Understudy) metric implementation.\n",
    "    Focuses on precision - how much of generated text appears in reference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_n:int = 4):\n",
    "        self.max_n = max_n \n",
    "\n",
    "    def _get_ngrams(self, tokens:List[str], n:int) -> Counter:\n",
    "        \"\"\"Extract n-grams from the token list\"\"\"\n",
    "        if n > len(tokens):\n",
    "            return Counter()\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            ngrams.append(tuple(tokens[i:i+n])) \n",
    "        return Counter(ngrams) \n",
    "    \n",
    "\n",
    "    def _modified_precision(self, candidate:List[str], references:List[List[str]],n:int):\n",
    "        \"\"\"Calculate modified n-gram precisions\"\"\"\n",
    "        candidate_ngrams = self._get_ngrams(candidate, n) \n",
    "        if not candidate_ngrams:\n",
    "            return 0.0 \n",
    "        \n",
    "        ## get maximum count for each n-gram across all references\n",
    "        max_ref_counts = Counter() \n",
    "        for reference in references:\n",
    "            ref_ngrams = self._get_ngrams(reference, n) \n",
    "            for ngram in candidate_ngrams:\n",
    "                max_ref_counts[ngram] = max(max_ref_counts[ngram], ref_ngrams[ngram]) \n",
    "        \n",
    "        # calculate clipped count\n",
    "        clipped_count = 0\n",
    "        total_counts = 0\n",
    "        for ngram, count in candidate_ngrams.items():\n",
    "            clipped_count+= min(count, max_ref_counts[ngram])\n",
    "            total_counts+=count \n",
    "\n",
    "        return clipped_count/ total_counts if total_counts > 0 else 0.0 \n",
    "    \n",
    "\n",
    "    def _brevity_penalty(self, candidate_len:int, reference_len:List[int]):\n",
    "        \"\"\"calculate brevity penalty\"\"\"\n",
    "        closest_ref_len = min(reference_len, key=lambda x: abs(x-candidate_len)) \n",
    "        if candidate_len > closest_ref_len :\n",
    "            return 1.0 \n",
    "        else:\n",
    "            return math.exp(1-closest_ref_len/candidate_len) if candidate_len > 0 else 0.0 \n",
    "        \n",
    "\n",
    "    ## calculate the blue score     \n",
    "    def calculate(self, candidate:str, references:List[str]):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score.\n",
    "        \n",
    "        Args:\n",
    "            candidate: Generated text\n",
    "            references: List of reference texts\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with BLEU score and component metrics\n",
    "        \"\"\"\n",
    "        candidate_tokens = candidate.lower().split() \n",
    "        references_tokens = [ref.lower().split() for ref in references] \n",
    "\n",
    "        if not candidate_tokens:\n",
    "            return {'blue': 0.0, 'precision':[0.0]*self.max_n, 'brevity_penalty': 0.0} \n",
    "        \n",
    "        # calculate modified precisions\n",
    "        precisions = [] \n",
    "        for n in range(1, self.max_n+1):\n",
    "            precision = self._modified_precision(candidate_tokens, references_tokens, n) \n",
    "            precisions.append(precision) \n",
    "\n",
    "        # calcualte brevity penlty\n",
    "        candidate_len = len(candidate_tokens) \n",
    "        reference_len = [len(ref) for ref in references_tokens] \n",
    "        bp = self._brevity_penalty(candidate_len, reference_len) \n",
    "\n",
    "        ## calculate blue score \n",
    "        if all(p > 0 for p in precisions):\n",
    "            log_precisions = [math.log(p) for p in precisions] \n",
    "            geometric_mean = math.exp(sum(log_precisions)/len(log_precisions)) \n",
    "            blue = bp*geometric_mean \n",
    "        else:\n",
    "            blue = 0.0 \n",
    "\n",
    "        return {\n",
    "            'blue' : blue,\n",
    "            'precisions' : precisions,\n",
    "            'candidate_len' : candidate_len,\n",
    "            'reference_len': reference_len\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb510ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue': 0.6197980942410934, 'precisions': [0.6666666666666666, 0.625, 0.5714285714285714], 'candidate_len': 9, 'reference_len': [6, 7]}\n"
     ]
    }
   ],
   "source": [
    "## example test\n",
    "candidate = \"The cat sat on the mat and looked around\" ## this is response from the model\n",
    "references_bleu = [\"The cat sat on the mat\", \"A cat was sitting on the mat\"] ## this is ground truth references \n",
    "\n",
    "bleu_scorer = BLEUScore(3)\n",
    "bleu_results = bleu_scorer.calculate(candidate, references_bleu)\n",
    "\n",
    "print(bleu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d7087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
